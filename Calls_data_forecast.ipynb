{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cc8a8b",
   "metadata": {},
   "source": [
    "## This code containts forecasts using time series methods (ES and AR) and ML methods (tree ensembles using GB and XGB) for Calls Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used python functions and display settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key imports for this code (various ML and Stat Models)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import viz libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "from plotly.graph_objs import *\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc807bb",
   "metadata": {},
   "source": [
    "### Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c184d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from the CSV file\n",
    "calls_df = pd.read_csv('calls_data.csv', parse_dates = ['date'])\n",
    "\n",
    "calls_df.head()\n",
    "calls_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot of calls made over time\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= calls_df['date'], y= calls_df['calls']))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls Made'), \n",
    "                   title = 'Time Series of Monthly Calls Made')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98169cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming into the 100th to 119th data points of above plot\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= calls_df['date'][100:120], y= calls_df['calls'][100:120]))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls Made'), \n",
    "                   title = 'Time Series of Monthly Calls Made')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing above plot using dots\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= calls_df['date'][100:120], y= calls_df['calls'][100:120], mode = 'markers'))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls Made'), \n",
    "                   title = 'Time Series of Monthly Calls Made')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5117119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a graph of the autocorrelation function versus lags for the calls data\n",
    "sm.graphics.tsa.plot_acf(calls_df['calls'].values.squeeze(), lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a graph of the partial autocorrelation function versus lags for the calls data\n",
    "sm.graphics.tsa.plot_pacf(calls_df['calls'].values.squeeze(), lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2580ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to only include 300 data points (most recent)\n",
    "# ES and ARIMA methods are better with < 200 training data points\n",
    "\n",
    "subset_data = calls_df[calls_df['date'] > '1994-01-01'].reset_index(drop = True)\n",
    "len(subset_data)\n",
    "subset_data.head()\n",
    "subset_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c0137",
   "metadata": {},
   "source": [
    "## Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the time series\n",
    "time_series = subset_data['calls']\n",
    "\n",
    "# Define the seasonal periods\n",
    "seasonal_periods = 12 \n",
    "\n",
    "# Define the number of predictions to make \n",
    "h = 1\n",
    "\n",
    "# Define the length of each training set\n",
    "T = 200\n",
    "\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "perc_error_list = []\n",
    "abs_error_list = []\n",
    "\n",
    "es_preds_train = np.zeros(T+h) # In case we wish to use the ES predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using exponential smoothing\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "    \n",
    "    # Fit the exponential smoothing model\n",
    "    model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=seasonal_periods)\n",
    "    fit_model = model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(h)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    perc_error_list.append(perc_errors)\n",
    "    abs_error_list.append(abs_errors)\n",
    "    \n",
    "    # Get the ES predictions\n",
    "    es_preds_train = np.append(es_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c16221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "avg_global = subset_data['calls'][T+h:].mean()\n",
    "print('Mean absolute error ratio:', np.mean(abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e46922",
   "metadata": {},
   "source": [
    "## SARIMAX initial model building and then train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting optimal differencing \n",
    "d_opt = pm.arima.ndiffs(subset_data['calls'].iloc[0:T])\n",
    "d_opt\n",
    "D_opt = pm.arima.nsdiffs(subset_data['calls'].iloc[0:T], m = seasonal_periods)\n",
    "D_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df45931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical study of the effect of differencing\n",
    "subset_data['diff1'] = subset_data['calls'] - subset_data['calls'].shift(periods = 1)\n",
    "subset_data['diff2'] = subset_data['diff1'] - subset_data['diff1'].shift(periods = 1)\n",
    "subset_data.head()\n",
    "\n",
    "sm.graphics.tsa.plot_acf(subset_data['calls'].values.squeeze(), lags=40)\n",
    "sm.graphics.tsa.plot_acf(subset_data['diff1'].dropna().values.squeeze(), lags=40)\n",
    "sm.graphics.tsa.plot_acf(subset_data['diff2'].dropna().values.squeeze(), lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That we just to show, now we need to remove the columns we created in the previous cell\n",
    "subset_data.drop(columns = ['diff1', 'diff2'], inplace = True)\n",
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the percentage and absolute errors\n",
    "ar_perc_error_list = []\n",
    "ar_abs_error_list = []\n",
    "\n",
    "ar_preds_train = np.zeros(T+h) # In case we wish to use the ARIMA predictions\n",
    "\n",
    "# Loop through the data frame and make predictions using ARIMA\n",
    "for i in range(len(time_series) - T - h):\n",
    "    # Define the training and testing data sets\n",
    "    train = time_series.iloc[i:i+T].values\n",
    "    test = time_series.iloc[i+T:i+T+h].values\n",
    "\n",
    "    # Using a specified order (this would need to be fine-tuned)\n",
    "    order = (1, 1, 0) \n",
    "    seasonal_order = (1, 0, 0, seasonal_periods) \n",
    "\n",
    "    # Fit the SARIMAX or ARIMA model\n",
    "    model = SARIMAX(endog=train, exog=None, order=order, seasonal_order=seasonal_order)\n",
    "    fit_model = model.fit(disp=False)\n",
    "\n",
    "    # Make predictions\n",
    "    pred_list = fit_model.forecast(steps=len(test), exog=None)\n",
    "    preds = pred_list[h-1]\n",
    "\n",
    "    # Calculate percentage and absolute errors\n",
    "    ar_perc_errors = np.abs(test[h-1]-preds)/test[h-1]\n",
    "    ar_abs_errors = np.abs(test[h-1]-preds)\n",
    "\n",
    "    # Store the percentage and absolute errors\n",
    "    ar_perc_error_list.append(ar_perc_errors)\n",
    "    ar_abs_error_list.append(ar_abs_errors)\n",
    "    \n",
    "    # Get the ARIMA predictions\n",
    "    ar_preds_train = np.append(ar_preds_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060673f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(ar_perc_error_list, axis = 0))\n",
    "print('Median absolute percentage error:', np.median(ar_perc_error_list, axis = 0))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 75, axis = 0))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(ar_perc_error_list, 90, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c672ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "print('Mean absolute error ratio:', np.mean(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(ar_abs_error_list, axis = 0)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 75, axis = 0)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(ar_abs_error_list, 90, axis = 0)/avg_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d9911",
   "metadata": {},
   "source": [
    "### For Gradient Boosting we first do some trend adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a98cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ML Models we use the dates to obtain the month and year as features\n",
    "subset_data['month'] = subset_data['date'].dt.month\n",
    "subset_data['year'] = subset_data['date'].dt.year\n",
    "subset_data.head()\n",
    "subset_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the trend, we average across all months of a year (using only training data)\n",
    "yearly_avg = subset_data[['calls', 'year']].iloc[0:T].groupby(['year']).mean()\n",
    "yearly_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can bring the year back as a feature (above it is in the index, so we reset it)\n",
    "yearly_avg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model to fit a linear trend model\n",
    "\n",
    "model = LinearRegression(fit_intercept = True)\n",
    "model.fit(yearly_avg['year'].array.reshape(-1, 1), yearly_avg['calls']) # When extending to multiple features remove .array.reshape(-1, 1)\n",
    "\n",
    "# The following gives the R-square score\n",
    "model.score(yearly_avg['year'].array.reshape(-1, 1), yearly_avg['calls'])\n",
    "\n",
    "# This is the coefficient Beta_1 (or slope of the Simple Linear Regression line)\n",
    "model.coef_\n",
    "\n",
    "# This is the coefficient Beta_0\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the trend for the test and train dataset across all months\n",
    "subset_data['trend'] = -2817.955 + 1.4995*(subset_data['year']+subset_data['month']/12)\n",
    "subset_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the trend line (regression line) along with the monthly calls data\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= subset_data['date'], y= subset_data['calls'], name = 'calls'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'], y= subset_data['trend'], name = 'trend'))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls Made and Trend'), \n",
    "                   title = 'Time Series of Monthly Calls Made with Trend')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get some of the lag features later we would need the full data set, and extract the date and year; trend\n",
    "calls_df['month'] = calls_df['date'].dt.month\n",
    "calls_df['year'] = calls_df['date'].dt.year\n",
    "calls_df['trend'] = -2817.955 + 1.4995*(calls_df['year']+calls_df['month']/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals (de-trending) and shifting them to get lags by 1 month and 12 months\n",
    "subset_data['resid_lags'] = (calls_df['calls']-calls_df['trend']).shift(periods = \n",
    "                                                seasonal_periods)[-len(subset_data):].reset_index(drop = True)\n",
    "subset_data['resid_lag1'] = (calls_df['calls']-calls_df['trend']).shift(periods = \n",
    "                                                        1)[-len(subset_data):].reset_index(drop = True)\n",
    "subset_data.head(15)\n",
    "subset_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2efdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the training data (note: unlike ES and ARIMA, here we do not retrain)\n",
    "X_train = subset_data[0:T]\n",
    "X_train['residual'] = X_train['calls'] - X_train['trend']\n",
    "X_train.head()\n",
    "X_train.tail()\n",
    "# The \"y\" variable that we predict is the residual itself\n",
    "y_train = X_train['residual']\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining 100 data points are test data\n",
    "X_test = subset_data[T:]\n",
    "X_test.head()\n",
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da76d9",
   "metadata": {},
   "source": [
    "## GB (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef736115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just so we have the test and training data for the GB to choose the right features in the next cell\n",
    "X_train\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3188e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training (i.e., no re-training)\n",
    "\n",
    "# defining the model and parameters\n",
    "gb = GradientBoostingRegressor(n_estimators = 100, max_depth = 6, min_samples_leaf = 2)\n",
    "\n",
    "# Asking the model to fit the training data (features used: month, resid_lags, resid_lag1)\n",
    "gb = gb.fit(X_train.drop(columns = ['date', 'calls', 'year', 'trend', 'residual']), y_train) \n",
    "\n",
    "# Asking what the importance of features\n",
    "gb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for one shot training\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "gb_perc_error_list = []\n",
    "gb_abs_error_list = []\n",
    "\n",
    "# Make predictions using Gradient Boosting with a single run of train\n",
    "\n",
    "y_test = X_test['calls'] # This is the final prediction that we will compare against\n",
    "\n",
    "# Make predictions (residuals plus trend)\n",
    "y_preds = gb.predict(X_test.drop(columns = ['date', 'calls', 'year', 'trend'])) + X_test['trend']\n",
    "\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)\n",
    "\n",
    "# Store the percentage and absolute errors\n",
    "gb_perc_error_list.append(perc_errors)\n",
    "gb_abs_error_list.append(abs_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(gb_perc_error_list))\n",
    "print('Median absolute percentage error:', np.median(gb_perc_error_list))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(gb_perc_error_list, 75))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(gb_perc_error_list, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e82ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "print('Mean absolute error ratio:', np.mean(gb_abs_error_list)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(gb_abs_error_list)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(gb_abs_error_list, 75)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(gb_abs_error_list, 90)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the forecasts against the actuals, graphically\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:], y= subset_data['calls'][T+1:], name = 'calls'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:], y= y_preds[1:], name = 'GB', mode = 'markers'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:], y= es_preds_train[T+1:], name = 'ES', mode = 'markers'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:], y= ar_preds_train[T+1:], name = 'AR', mode = 'markers'))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls made versus predicted'), \n",
    "                   title = 'Monthly Calls Test Data vs. Predictions')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73523c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model forecasts versus actuals across 2 years\n",
    "plot_data = []\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:T+25], y= subset_data['calls'][T+1:T+25], name = 'calls'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:T+25], y= y_preds[1:T+25], name = 'GB', mode = 'markers'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:T+25], y= es_preds_train[T+1:T+25], name = 'ES', mode = 'markers'))\n",
    "plot_data.append(go.Scatter(x= subset_data['date'][T+1:T+25], y= ar_preds_train[T+1:T+25], name = 'AR', mode = 'markers'))\n",
    "layout = go.Layout(xaxis = dict(title='Date'), yaxis = dict(title= 'Calls made versus predicted'), \n",
    "                   title = 'Monthly Calls Test Data vs. Predictions')\n",
    "fig = go.Figure(data= plot_data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758076c",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we simply use the parameters without hyper parameter tuning\n",
    "# One shot training\n",
    "\n",
    "# Define the XGBoost regressor with specific hyperparameters\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Asking the model to fit the training data\n",
    "model.fit(X_train.drop(columns = ['date', 'calls', 'year', 'trend', 'residual']), y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efea643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot training\n",
    "# Initialize the lists to store the percentage and absolute errors\n",
    "xgb_perc_error_list = []\n",
    "xgb_abs_error_list = []\n",
    "\n",
    "# Make predictions using Xtreme Gradient Boosting with a single run of train\n",
    "\n",
    "y_test = X_test['calls']\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test.drop(columns = ['date', 'calls', 'year', 'trend'])) + X_test['trend']\n",
    "\n",
    "# Calculate percentage and absolute errors\n",
    "perc_errors = np.abs(y_test-y_preds)/y_test\n",
    "abs_errors = np.abs(y_test-y_preds)\n",
    "\n",
    "# Store the percentage and absolute errors\n",
    "xgb_perc_error_list.append(perc_errors)\n",
    "xgb_abs_error_list.append(abs_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd013c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage-error results\n",
    "print('Mean absolute percentage error:', np.mean(xgb_perc_error_list, axis = 1))\n",
    "print('Median absolute percentage error:', np.median(xgb_perc_error_list, axis = 1))\n",
    "print('75th percentile of absolute percentage error:', np.percentile(xgb_perc_error_list, 75, axis = 1))\n",
    "print('90th percentile of absolute percentage error:', np.percentile(xgb_perc_error_list, 90, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the absolute error ratio results\n",
    "print('Mean absolute error ratio:', np.mean(xgb_abs_error_list, axis = 1)/avg_global)\n",
    "print('Median absolute error ratio:', np.median(xgb_abs_error_list, axis = 1)/avg_global)\n",
    "print('75th percentile absolute error ratio:', np.percentile(xgb_abs_error_list, 75, axis = 1)/avg_global)\n",
    "print('90th percentile absolute error ratio:', np.percentile(xgb_abs_error_list, 90, axis = 1)/avg_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd1519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
